getwd()
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
#install dependency for metagear - EBImage
#source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
#install.packages('revtools')
library(revtools)
#install.packages('topicmodels')
library(topicmodels)
#install.packages('plyr')
library(plyr)
#install.packages('reshape')
library(reshape)
#install.packages('metagear')
library(metagear)
#install.packages('randomForest')
library(randomForest)
# LOAD BIBLIOS AND CLEAN --------------------------------------------------
#import SCOPUS bib file and convert to df
scopus_biblio_1<-as.data.frame(read_bibliography('input_data/scopus_1to676_130818.bib')) #import biblio
final_scopus_biblio.080818<-scopus_biblio_1 #compile final biblio (in this case just one file)
final_scopus_biblio.080818$abstract[which(is.na(final_scopus_biblio.080818$abstract))]<-"" #turn NA abstracts to empty space
final_scopus_biblio.080818$source<-'scopus' #label with source for downstream tracking
#import WoS bib file and convert to df
wos_biblio_1<-as.data.frame(read_bibliography('input_data/wos_1to500_130818.bib'))
wos_biblio_2<-as.data.frame(read_bibliography('input_data/wos_501-832_130818.bib'))
colnames.dup<-count(c(colnames(wos_biblio_1),colnames(wos_biblio_2)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==2])
final_wos_biblio.080818<-rbind(wos_biblio_1[,matchingcols],wos_biblio_2[,matchingcols])
final_wos_biblio.080818$abstract[which(is.na(final_wos_biblio.080818$abstract))]<-""
final_wos_biblio.080818$source<-'wos'
#import pubmed nbib files and convert to df
pubmed_biblio_1<-as.data.frame(read_bibliography('input_data/pubmed_1to200_130818.nbib'))
pubmed_biblio_2<-as.data.frame(read_bibliography('input_data/pubmed_201to400_130818.nbib'))
pubmed_biblio_3<-as.data.frame(read_bibliography('input_data/pubmed_400to541_130818.nbib'))
colnames.dup<-count(c(colnames(pubmed_biblio_1),colnames(pubmed_biblio_2),colnames(pubmed_biblio_3)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==3])
final_pubmed_biblio.080818<-rbind(pubmed_biblio_1[,matchingcols],pubmed_biblio_2[,matchingcols],pubmed_biblio_3[,matchingcols])
final_pubmed_biblio.080818$abstract[which(is.na(final_pubmed_biblio.080818$abstract))]<-""
final_pubmed_biblio.080818$source<-'pubmed'
colnames.dup.all<-count(c(colnames(final_scopus_biblio.080818),colnames(final_wos_biblio.080818),colnames(final_pubmed_biblio.080818)))
matchingcols<-as.character(colnames.dup.all$x[colnames.dup.all$freq==3])
masterbiblio.080818<-rbind(final_scopus_biblio.080818[,matchingcols],final_wos_biblio.080818[,matchingcols],final_pubmed_biblio.080818[,matchingcols])
count(is.na(masterbiblio.080818$abstract))
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
#install dependency for metagear - EBImage
#source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
#install.packages('revtools')
library(revtools)
#install.packages('topicmodels')
library(topicmodels)
#install.packages('plyr')
library(plyr)
#install.packages('reshape')
library(reshape)
#install.packages('metagear')
library(metagear)
#install.packages('randomForest')
library(randomForest)
# LOAD BIBLIOS AND CLEAN --------------------------------------------------
#import SCOPUS bib file and convert to df
scopus_biblio_1<-as.data.frame(read_bibliography('input_data/scopus_1to676_130818.bib')) #import biblio
final_scopus_biblio.080818<-scopus_biblio_1 #compile final biblio (in this case just one file)
final_scopus_biblio.080818$abstract[which(is.na(final_scopus_biblio.080818$abstract))]<-"" #turn NA abstracts to empty space
final_scopus_biblio.080818$source<-'scopus' #label with source for downstream tracking
#import WoS bib file and convert to df
wos_biblio_1<-as.data.frame(read_bibliography('input_data/wos_1to500_130818.bib'))
wos_biblio_2<-as.data.frame(read_bibliography('input_data/wos_501-832_130818.bib'))
colnames.dup<-count(c(colnames(wos_biblio_1),colnames(wos_biblio_2)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==2])
final_wos_biblio.080818<-rbind(wos_biblio_1[,matchingcols],wos_biblio_2[,matchingcols])
final_wos_biblio.080818$abstract[which(is.na(final_wos_biblio.080818$abstract))]<-""
final_wos_biblio.080818$source<-'wos'
#import pubmed nbib files and convert to df
pubmed_biblio_1<-as.data.frame(read_bibliography('input_data/pubmed_1to200_130818.nbib'))
pubmed_biblio_2<-as.data.frame(read_bibliography('input_data/pubmed_201to400_130818.nbib'))
pubmed_biblio_3<-as.data.frame(read_bibliography('input_data/pubmed_400to541_130818.nbib'))
colnames.dup<-count(c(colnames(pubmed_biblio_1),colnames(pubmed_biblio_2),colnames(pubmed_biblio_3)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==3])
final_pubmed_biblio.080818<-rbind(pubmed_biblio_1[,matchingcols],pubmed_biblio_2[,matchingcols],pubmed_biblio_3[,matchingcols])
final_pubmed_biblio.080818$abstract[which(is.na(final_pubmed_biblio.080818$abstract))]<-""
final_pubmed_biblio.080818$source<-'pubmed'
#bind all dataframes tgoether without pubmed to create master biblio
#colnames.dup.all<-count(c(colnames(final_scopus_biblio.080818),colnames(final_wos_biblio.080818)))
#matchingcols<-as.character(colnames.dup.all$x[colnames.dup.all$freq==2])
#masterbiblio.080818<-rbind(final_scopus_biblio.080818[,matchingcols],final_wos_biblio.080818[,matchingcols])
#count(is.na(masterbiblio.080818$abstract))
#bind all dataframes tgoether with pubmed to create master biblio
colnames.dup.all<-count(c(colnames(final_scopus_biblio.080818),colnames(final_wos_biblio.080818),colnames(final_pubmed_biblio.080818)))
matchingcols<-as.character(colnames.dup.all$x[colnames.dup.all$freq==3])
masterbiblio.080818<-rbind(final_scopus_biblio.080818[,matchingcols],final_wos_biblio.080818[,matchingcols],final_pubmed_biblio.080818[,matchingcols])
count(is.na(masterbiblio.080818$abstract))
# REVTOOLS PROCESSING -----------------------------------------------------
#remove duplicates
dupsfinal.080818<-find_duplicates(masterbiblio.080818)
masterbiblio.080818_dupsremoved<-extract_unique_references(dupsfinal.080818)
#remove duplicates double check by removing duplicated dois
doi.dups<-which(duplicated(masterbiblio.080818_dupsremoved$doi, incomparables = NA))
masterbiblio.080818_dupsremoved<-masterbiblio.080818_dupsremoved[-doi.dups,]
# TRAINING: FINDING RELEVANT LITERATURE ------------------------------------------
#GOAL: We have lots of literature, dominated by medical stuff.
#Some of this medical stuff is about hospital air, so is *kind of* relevant.
#Let's use the abstract screener to separate up this literature into (don't use air terms as in all):
#YES IT'S RELEVANT! (YES): DIRECTLY Environmental (non-hospital) AMR e.g. case studies, disease descriptions
#MAYBE OF RELEVANCE (MAYBE): DIRECTLY About Environmental AMR, but within a hospital context.
#NO RELEVANCE (NO): Not DIRECTLY about environmental AMR AND/OR AIR
#check what effort files you have before removing
list.files(pattern = "effort")
#do.call(file.remove, list(list.files(pattern="effort")))
list.files(pattern = "effort")
set.seed(1235)
topic.temp<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
set.seed(1235)
masterbiblio.080818_dupsremoved<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
#set up stuff for metagear abstract screening - correcting and matching column titles and order etc
colnames(topic.temp)
topic.temp$pages<-"1-?"
LPAGES_UPAGES<-t(as.data.frame(strsplit(topic.temp$pages,'-')))
colnames(LPAGES_UPAGES)<-c('LPAGES','UPAGES')
topic.temp<-cbind(topic.temp,LPAGES_UPAGES)
metagear_topic.temp<-topic.temp[,c("author","year","title","journal","volume","LPAGES","UPAGES","doi","abstract")]
colnames(metagear_topic.temp)<-colnames(example_references_metagear)
list.files(pattern = "effort")
list.files(pattern = "effort")
set.seed(1235)
topic.temp<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
list.files(pattern = "effort")
list.files(pattern = "effort")
set.seed(1235)
topic.temp<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
#install dependency for metagear - EBImage
#source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
#install.packages('revtools')
library(revtools)
#install.packages('topicmodels')
library(topicmodels)
#install.packages('plyr')
library(plyr)
#install.packages('reshape')
library(reshape)
#install.packages('metagear')
library(metagear)
#install.packages('randomForest')
library(randomForest)
# LOAD BIBLIOS AND CLEAN --------------------------------------------------
#import SCOPUS bib file and convert to df
scopus_biblio_1<-as.data.frame(read_bibliography('input_data/scopus_1to676_130818.bib')) #import biblio
final_scopus_biblio.080818<-scopus_biblio_1 #compile final biblio (in this case just one file)
final_scopus_biblio.080818$abstract[which(is.na(final_scopus_biblio.080818$abstract))]<-"" #turn NA abstracts to empty space
final_scopus_biblio.080818$source<-'scopus' #label with source for downstream tracking
#import WoS bib file and convert to df
wos_biblio_1<-as.data.frame(read_bibliography('input_data/wos_1to500_130818.bib'))
wos_biblio_2<-as.data.frame(read_bibliography('input_data/wos_501-832_130818.bib'))
colnames.dup<-count(c(colnames(wos_biblio_1),colnames(wos_biblio_2)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==2])
final_wos_biblio.080818<-rbind(wos_biblio_1[,matchingcols],wos_biblio_2[,matchingcols])
final_wos_biblio.080818$abstract[which(is.na(final_wos_biblio.080818$abstract))]<-""
final_wos_biblio.080818$source<-'wos'
#import pubmed nbib files and convert to df
pubmed_biblio_1<-as.data.frame(read_bibliography('input_data/pubmed_1to200_130818.nbib'))
pubmed_biblio_2<-as.data.frame(read_bibliography('input_data/pubmed_201to400_130818.nbib'))
pubmed_biblio_3<-as.data.frame(read_bibliography('input_data/pubmed_400to541_130818.nbib'))
colnames.dup<-count(c(colnames(pubmed_biblio_1),colnames(pubmed_biblio_2),colnames(pubmed_biblio_3)))
matchingcols<-as.character(colnames.dup$x[colnames.dup$freq==3])
final_pubmed_biblio.080818<-rbind(pubmed_biblio_1[,matchingcols],pubmed_biblio_2[,matchingcols],pubmed_biblio_3[,matchingcols])
final_pubmed_biblio.080818$abstract[which(is.na(final_pubmed_biblio.080818$abstract))]<-""
final_pubmed_biblio.080818$source<-'pubmed'
#bind all dataframes tgoether without pubmed to create master biblio
#colnames.dup.all<-count(c(colnames(final_scopus_biblio.080818),colnames(final_wos_biblio.080818)))
#matchingcols<-as.character(colnames.dup.all$x[colnames.dup.all$freq==2])
#masterbiblio.080818<-rbind(final_scopus_biblio.080818[,matchingcols],final_wos_biblio.080818[,matchingcols])
#count(is.na(masterbiblio.080818$abstract))
#bind all dataframes tgoether with pubmed to create master biblio
colnames.dup.all<-count(c(colnames(final_scopus_biblio.080818),colnames(final_wos_biblio.080818),colnames(final_pubmed_biblio.080818)))
matchingcols<-as.character(colnames.dup.all$x[colnames.dup.all$freq==3])
masterbiblio.080818<-rbind(final_scopus_biblio.080818[,matchingcols],final_wos_biblio.080818[,matchingcols],final_pubmed_biblio.080818[,matchingcols])
count(is.na(masterbiblio.080818$abstract))
dupsfinal.080818<-find_duplicates(masterbiblio.080818)
masterbiblio.080818_dupsremoved<-extract_unique_references(dupsfinal.080818)
?extract_unique_references
masterbiblio.080818_dupsremoved<-extract_unique_references(masterbiblio.090818,dupsfinal.080818)
masterbiblio.080818_dupsremoved<-extract_unique_references(masterbiblio.080818,dupsfinal.080818)
doi.dups<-which(duplicated(masterbiblio.080818_dupsremoved$doi, incomparables = NA))
masterbiblio.080818_dupsremoved<-masterbiblio.080818_dupsremoved[-doi.dups,]
list.files(pattern = "effort")
list.files(pattern = "effort")
set.seed(1235)
topic.temp<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
set.seed(1235)
masterbiblio.080818_dupsremoved<-masterbiblio.080818_dupsremoved[sample(1:nrow(masterbiblio.080818_dupsremoved)),]
colnames(topic.temp)
topic.temp$pages<-"1-?"
LPAGES_UPAGES<-t(as.data.frame(strsplit(topic.temp$pages,'-')))
colnames(LPAGES_UPAGES)<-c('LPAGES','UPAGES')
topic.temp<-cbind(topic.temp,LPAGES_UPAGES)
metagear_topic.temp<-topic.temp[,c("author","year","title","journal","volume","LPAGES","UPAGES","doi","abstract")]
colnames(metagear_topic.temp)<-colnames(example_references_metagear)
effort_distribute(metagear_topic.temp, initialize = TRUE, reviewers = "envamr", save_split = TRUE)
?effort_distribute
effort_distribute(metagear_topic.temp, initialize = TRUE, reviewers = "envamr", save_split = TRUE, directory="output_data")
effort_distribute(metagear_topic.temp, initialize = TRUE, reviewers = "envamr", save_split = TRUE, directory="output_data")
abstract_screener('/output_data/effort_envamr.csv', aReviewer = 'envamr')
abstract_screener('output_data/effort_envamr.csv', aReviewer = 'envamr')
?list.files
list.files("output_data",pattern = "effort")
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
# INSTALL NECESSARY PACKAGES ----------------------------------------------
#install.packages("plyr") - count function needed (not to be confused with base count function)
library(plyr)
#install.packages('meta') - for metaprop meta-analysis
library(meta)
#install.packages('scale') - for viewing colour palettes
library(scales)
#install.packages('randomForest') - for random forest analysis (randomForest function etc.)
library(randomForest)
#install.packages('caret') - for varImp function to display varImps of randomForest object
library(caret)
#install.packages('rpart') - for regression tree (rpart function)
library(rpart)
#install.packages('ggsci') - for regression tree (rpart function)
library(ggsci)
# LOAD FUNCTIONS ----------------------------------------------------------
#load function to calculate standard error (to be used in graph error bars)
se <- function(x) sd(x, na.rm = T)/sqrt(length(x))
# LOAD DATA ---------------------------------------------------------------
#load full bibliographic dataframe of all studies used in the lit review (though not necessarily quantitatively)
fullbiblio<-read.csv('input_data/biblio_all_final.csv')
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
# INSTALL NECESSARY PACKAGES ----------------------------------------------
#install.packages("plyr") - count function needed (not to be confused with base count function)
library(plyr)
#install.packages('meta') - for metaprop meta-analysis
library(meta)
#install.packages('scale') - for viewing colour palettes
library(scales)
#install.packages('randomForest') - for random forest analysis (randomForest function etc.)
library(randomForest)
#install.packages('caret') - for varImp function to display varImps of randomForest object
library(caret)
#install.packages('rpart') - for regression tree (rpart function)
library(rpart)
#install.packages('ggsci') - for regression tree (rpart function)
library(ggsci)
# LOAD FUNCTIONS ----------------------------------------------------------
#load function to calculate standard error (to be used in graph error bars)
se <- function(x) sd(x, na.rm = T)/sqrt(length(x))
# LOAD DATA ---------------------------------------------------------------
#load full bibliographic dataframe of all studies used in the lit review (though not necessarily quantitatively)
fullbiblio<-read.csv('input_data/biblio_all_final.csv')
getwd()
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
# INSTALL NECESSARY PACKAGES ----------------------------------------------
#install.packages("plyr") - count function needed (not to be confused with base count function)
library(plyr)
#install.packages('meta') - for metaprop meta-analysis
library(meta)
#install.packages('scale') - for viewing colour palettes
library(scales)
#install.packages('randomForest') - for random forest analysis (randomForest function etc.)
library(randomForest)
#install.packages('caret') - for varImp function to display varImps of randomForest object
library(caret)
#install.packages('rpart') - for regression tree (rpart function)
library(rpart)
#install.packages('ggsci') - for regression tree (rpart function)
library(ggsci)
# LOAD FUNCTIONS ----------------------------------------------------------
#load function to calculate standard error (to be used in graph error bars)
se <- function(x) sd(x, na.rm = T)/sqrt(length(x))
# LOAD DATA ---------------------------------------------------------------
#load full bibliographic dataframe of all studies used in the lit review (though not necessarily quantitatively)
fullbiblio<-read.csv('analysis/input_data/biblio_all_final.csv')
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
#install dependency for metagear - EBImage
#source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
#install.packages('revtools')
library(revtools)
#install.packages('topicmodels')
library(topicmodels)
#install.packages('plyr')
library(plyr)
#install.packages('reshape')
library(reshape)
#install.packages('metagear')
library(metagear)
#install.packages('randomForest')
library(randomForest)
# LOAD BIBLIOS AND CLEAN --------------------------------------------------
#import SCOPUS bib file and convert to df
scopus_biblio_1<-as.data.frame(read_bibliography('screening/input_data/scopus_1to676_130818.bib')) #import biblio
# START -------------------------------------------------------------------
rm(list=ls())
set.seed(1234)
# INSTALL NECESSARY PACKAGES ----------------------------------------------
#install.packages("plyr") - count function needed (not to be confused with base count function)
library(plyr)
#install.packages('meta') - for metaprop meta-analysis
library(meta)
#install.packages('scale') - for viewing colour palettes
library(scales)
#install.packages('randomForest') - for random forest analysis (randomForest function etc.)
library(randomForest)
#install.packages('caret') - for varImp function to display varImps of randomForest object
library(caret)
#install.packages('rpart') - for regression tree (rpart function)
library(rpart)
#install.packages('ggsci') - for regression tree (rpart function)
library(ggsci)
# LOAD FUNCTIONS ----------------------------------------------------------
#load function to calculate standard error (to be used in graph error bars)
se <- function(x) sd(x, na.rm = T)/sqrt(length(x))
# LOAD DATA ---------------------------------------------------------------
#load full bibliographic dataframe of all studies used in the lit review (though not necessarily quantitatively)
fullbiblio<-read.csv('analysis/input_data/biblio_all_final.csv')
